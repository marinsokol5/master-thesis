{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/mnt/c/cloud/thesis/\")\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tokenizers\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15, 10)})\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from constants import paths as p\n",
    "from constants import tokens as t\n",
    "from constants import hyperparameters as hp\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.callbacks as cb\n",
    "from models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = nn.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = torch.zeros((32, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask[:, 18:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = t.forward(src, tgt, tgt_key_padding_mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size x sequence_length(padded with a certain index) x vocabulary_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs say: batch_size x vocabulary_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32, 256, 100).transpose(1, 2)\n",
    "target = torch.empty(32, 256, dtype=torch.long).random_(100)\n",
    "target[:, 50:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100, 256]), torch.Size([32, 256]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1161)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1420)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8711, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2.forward(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDB_Reviews(pl.LightningDataModule):\n",
    "    def __init__(self, train_path=p.TRAIN_TENSOR_DATASET_PATH, validaiton_path=p.VALIDATION_TENSOR_DATASET_PATH, batch_size=hp.BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.validation_path = validaiton_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.train_data = torch.load(self.train_path)\n",
    "        self.validation_data = torch.load(self.validation_path)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validation_data, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-a0c833a48a3c>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-a0c833a48a3c>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    def forward(batch, batch_idx)\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class TransformerLightning(pl.LightningModule):\n",
    "    def __init__(self, vocabulary_size=hp.VOCABULARY_SIZE, embedding_size=hp.EMBEDDING_SIZE, number_of_properties=hp.NUMBER_OF_PROPERTIES, padding_index=hp.PADDING_INDEX, model_dimension=hp.MODEL_DIMENSION, target_sequence_length=hp.TARGET_SEQUENCE_LENGTH, dropout_probability=hp.DROPOUT_PROBABILITY, feed_forward_transformer_layer_dimension=hp.FEED_FORWARD_TRANSFORMER_LAYER_DIMENSION):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(vocabulary_size, embedding_size, number_of_properties, padding_index, model_dimension, target_sequence_length, dropout_probability, feed_forward_transformer_layer_dimension)\n",
    "        self.loss_function = nn.CrossEntropyLoss(ignore_index=t.PADDING_TOKEN)\n",
    "        \n",
    "        self.train_name = \"train\"\n",
    "        self.validation_name = \"validation\"\n",
    "        \n",
    "    def forward(self, batched_source_numericalized, batched_source_properties, batched_source_padding_mask):\n",
    "        pass\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=6*10e-5)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx, self.train_name)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx, self.validation_name)\n",
    "\n",
    "    def _step(self, batch, batch_idx, mode):\n",
    "        bsn, bsp, btin, bspm, btipm, bton = batch\n",
    "        logits = self.transformer(bsn, bsp, btin, bspm, btipm)\n",
    "        \n",
    "        # logits -> batch_size x target_sequence_length x vocabulary_size\n",
    "        # bton -> batch_size x target_sequence_length([0, vocabulary_size-1])\n",
    "        loss = self.loss_function(rearrange(logits, 'b t v -> b v t'), bton)\n",
    "\n",
    "        on_step = True if mode == self.train_name else False\n",
    "        self.log(f'{mode}_loss', loss, on_step=on_step, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
