{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/mnt/c/cloud/thesis/\")\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tokenizers\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15, 10)})\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from constants import paths as p\n",
    "from constants import tokens as t\n",
    "from constants import hyperparameters as hp\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.callbacks as cb\n",
    "from models.lightning import IMDB_Reviews, TransformerLightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html  \n",
    "https://github.com/abrazinskas/FewSum/tree/master/fewsum/modelling/models  \n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html  \n",
    "https://colab.research.google.com/drive/1Uq5vIheRUuRbCplQe29aeSAi_fe-76v_#scrollTo=5Ed0vgI0PupK  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = IMDB_Reviews(p.TRAIN_TENSOR_DATASET_PATH, p.VALIDATION_TENSOR_DATASET_PATH, hp.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLightning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLightning(\n",
       "  (transformer): Transformer(\n",
       "    (embedder): Embedding(20000, 400, padding_idx=3)\n",
       "    (embedder_drouput): Dropout(p=0.1, inplace=False)\n",
       "    (embedding_layer): Sequential(\n",
       "      (0): Embedding(20000, 400, padding_idx=3)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (combine_embeddings_and_properties_layer): Linear(in_features=405, out_features=400, bias=True)\n",
       "    (positional_encoding_layer): PositionalEncoding(\n",
       "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=400, out_features=400, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=400, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=1024, out_features=400, bias=True)\n",
       "            (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (transformer_to_vocabulary_logits_layer): Linear(in_features=400, out_features=20000, bias=True)\n",
       "  )\n",
       "  (loss_function): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "Cannot use GPUStatsMonitor callback because NVIDIA driver is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a058d5cd2507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar_refresh_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sanity_val_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPUStatsMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{epoch}_{validation_loss:.3f}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_top_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/gpu_stats_monitor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, memory_utilization, gpu_utilization, intra_step_time, inter_step_time, fan_speed, temperature)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nvidia-smi'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             raise MisconfigurationException(\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;34m'Cannot use GPUStatsMonitor callback because NVIDIA driver is not installed.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             )\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: Cannot use GPUStatsMonitor callback because NVIDIA driver is not installed."
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(progress_bar_refresh_rate=50, max_epochs=5, gpus=1, num_sanity_val_steps=1, callbacks=[cb.EarlyStopping(\"validation_loss\"), cb.GPUStatsMonitor(), cb.ModelCheckpoint(dirpath=\"./models\", filename='{epoch}_{validation_loss:.3f}', mode=\"min\", save_top_k=3, monitor=\"validation_loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
