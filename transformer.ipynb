{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://andrewpeng.dev/transformer-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/mnt/c/cloud/thesis/\")\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tokenizers\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15, 10)})\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from constants import paths as p\n",
    "from constants import tokens as t\n",
    "from constants import hyperparameters as hp\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.callbacks as cb\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(p.TRAIN_NUMERICALIZED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.read_pickle(p.VALIDATION_NUMERICALIZED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train_batch = validation.sample(n=hp.BATCH_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 27)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'input', 'output', 'helpfulness_difference', 'rating_difference',\n",
       "       'length_difference', 'target_string', 'source_string',\n",
       "       'source_numericalized', 'target_numericalized',\n",
       "       'source_numericalized_padded', 'target_numericalized_input',\n",
       "       'target_numericalized_output', 'target_numericalized_input_padded',\n",
       "       'target_numericalized_output_padded', 'source_start_token_indexes',\n",
       "       'source_end_token_indexes', 'source_padding_token_first_index',\n",
       "       'target_input_padding_token_first_index', 'source_padded_indices',\n",
       "       'source_padding_mask', 'target_input_padding_mask', 'source_ratings',\n",
       "       'source_helpfulnesses', 'helpfulness_difference_vector',\n",
       "       'rating_difference_vector', 'length_difference_vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_train_batch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_columns = [\"source_ratings\", \"source_helpfulnesses\", \"helpfulness_difference_vector\", \"rating_difference_vector\", \"length_difference_vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10]), torch.Size([3, 15]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n[0].shape, n[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_training_matrixes(data, file):\n",
    "    batched_source_numericalized = torch.from_numpy(np.stack(data.source_numericalized_padded)).long()\n",
    "    batched_source_properties = torch.from_numpy(np.stack([np.stack(data[p].to_numpy()) for p in properties_columns], axis=2)).float()\n",
    "    batched_target_input_numericalized = torch.from_numpy(np.stack(data.target_numericalized_input_padded)).long()\n",
    "    batched_source_padding_mask = torch.from_numpy(np.stack(data.source_padding_mask)).bool()\n",
    "    batched_target_input_padding_mask = torch.from_numpy(np.stack(data.target_input_padding_mask)).bool()\n",
    "    batched_target_output_numericalized = torch.from_numpy(np.stack(data.target_numericalized_output_padded)).long()\n",
    "    \n",
    "    torch.save(TensorDataset(batched_source_numericalized, batched_source_properties, batched_target_input_numericalized, batched_source_padding_mask, batched_target_input_padding_mask, batched_target_output_numericalized), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_training_matrixes(train, p.TRAIN_TENSOR_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_training_matrixes(validation, p.VALIDATION_TENSOR_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.load(p.TRAIN_TENSOR_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = torch.load(p.VALIDATION_TENSOR_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(validation, batch_size=hp.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 512]), 'torch.LongTensor'),\n",
       " (torch.Size([32, 512, 5]), 'torch.FloatTensor'),\n",
       " (torch.Size([32, 256]), 'torch.LongTensor'),\n",
       " (torch.Size([32, 512]), 'torch.BoolTensor'),\n",
       " (torch.Size([32, 256]), 'torch.BoolTensor'),\n",
       " (torch.Size([32, 256]), 'torch.LongTensor')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_and_type(*args):\n",
    "    result = []\n",
    "    for arg in args:\n",
    "        result.append((arg.shape, arg.type()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_source_numericalized = torch.from_numpy(np.stack(random_train_batch.source_numericalized_padded)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_target_input_numericalized = torch.from_numpy(np.stack(random_train_batch.target_numericalized_input_padded)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_source_properties = torch.from_numpy(np.stack([np.stack(random_train_batch[p].to_numpy()) for p in properties_columns], axis=2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_source_padding_mask = torch.from_numpy(np.stack(random_train_batch.source_padding_mask)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_target_input_padding_mask = torch.from_numpy(np.stack(random_train_batch.target_input_padding_mask)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 512]), 'torch.LongTensor'),\n",
       " (torch.Size([32, 512, 5]), 'torch.FloatTensor'),\n",
       " (torch.Size([32, 256]), 'torch.LongTensor'),\n",
       " (torch.Size([32, 512]), 'torch.BoolTensor'),\n",
       " (torch.Size([32, 256]), 'torch.BoolTensor')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(batched_source_numericalized, batched_source_properties, batched_target_input_numericalized, batched_source_padding_mask, batched_target_input_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched_source_numericalized -> batch_size x source_sequence_length(start_id, t1, t2, t3.., end_id, start_id, t4.., pad_id, pad_id..)(int/long)\n",
    "# batched_source_properties -> batch_size x source_sequence_length x 5(number_of_properties)(0 for special tokens...)(float)\n",
    "# batched_target_input_numericalized -> batch_size x target_sequence_length (start_id, t1, t2... pad_id, pad_id, pad_id...)(int/long)(without end_id)\n",
    "# batched_source/target_padding_mask -> batch_size x source/target_sequence_length(False, False..., True, True..)(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_source_embedded = self.embedding_layer(batched_source_numericalized)\n",
    "batched_target_embedded = self.embedding_layer(batched_target_input_numericalized)\n",
    "# batched_source/target_embedded -> batch_size x source/target_sequence_length x embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 512, 400]), 'torch.FloatTensor'),\n",
       " (torch.Size([32, 256, 400]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(batched_source_embedded, batched_target_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.MODEL_DIMENSION, hp.EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_source = torch.cat((batched_source_embedded, batched_source_properties), dim=2) \n",
    "# -> batch_size x source_sequence_length x embedding_size + number_of_properies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 512, 405]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(batched_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_source = self.combine_embeddings_and_properties_layer(batched_source)  \n",
    "# -> batch_size x source_sequence_length x model_dimension (512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 512, 400]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(batched_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 512, 400]), 'torch.FloatTensor'),\n",
       " (torch.Size([32, 256, 400]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(batched_source, batched_target_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([5000, 400]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(self.positional_encoding_layer.positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_source = self.positional_encoding_layer(batched_source * math.sqrt(self.model_dimension))  # normalizing(reducing variance) before positonally encoding\n",
    "batched_target = self.positional_encoding_layer(batched_target_embedded * math.sqrt(self.model_dimension))\n",
    "# batched_source/target -> batch_size x source/target_sequence_length x model_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 512, 400]), 'torch.FloatTensor'),\n",
       " (torch.Size([32, 256, 400]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(batched_source, batched_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 512]), 'torch.BoolTensor'),\n",
       " (torch.Size([32, 256]), 'torch.BoolTensor')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(batched_source_padding_mask, batched_target_input_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([256, 256]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(self.decoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_output = self.transformer(\n",
    "    src=rearrange(batched_source[:,:10,:], 'b s m -> s b m'),  # batched_source.tranpose(0, 1)\n",
    "    tgt=rearrange(batched_target[:,:20,:], 'b s m -> s b m'),\n",
    "    tgt_mask=self.decoder_attention_mask[:20,:20],\n",
    "    src_key_padding_mask=batched_source_padding_mask[:,:10],\n",
    "    memory_key_padding_mask=batched_source_padding_mask[:,:10],\n",
    "    tgt_key_padding_mask=batched_target_input_padding_mask[:,:20],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([20, 32, 400]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(transformer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer_output = self.transformer(\n",
    "    src=rearrange(batched_source, 'b s m -> s b m'),  # batched_source.tranpose(0, 1)\n",
    "    tgt=rearrange(batched_target, 'b s m -> s b m'),\n",
    "    tgt_mask=self.decoder_attention_mask,\n",
    "    src_key_padding_mask=batched_source_padding_mask,\n",
    "    memory_key_padding_mask=batched_source_padding_mask,\n",
    "    tgt_key_padding_mask=batched_target_input_padding_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_output = rearrange(transformer_output, 's b m -> b s m')\n",
    "# transformer_output -> batch_size x target_sequence_length x model_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 20, 400]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(transformer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_logits = self.transformer_to_vocabulary_logits_layer(transformer_output)\n",
    "# vocabulary_logits -> batch_size x target_sequence_length x vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 20, 20000]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(vocabulary_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.8783, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_logits.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9149, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_logits.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_log_probability = F.log_softmax(vocabulary_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 20, 20000]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(vocabulary_log_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-12.9500, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_log_probability.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.1453, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_log_probability.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could rename this as max target sequence lengthed or cached actually, same as positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(target_sequence_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([32, 20, 20000]), 'torch.FloatTensor')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_and_type(transformer.forward(\n",
    "    batched_source_numericalized[:,:10],\n",
    "    batched_source_properties[:,:10,:],\n",
    "    batched_target_input_numericalized[:,:20],\n",
    "    batched_source_padding_mask[:,:10], \n",
    "    batched_target_input_padding_mask[:,:20],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocabulary_size=hp.VOCABULARY_SIZE, embedding_size=hp.EMBEDDING_SIZE, number_of_properties=hp.NUMBER_OF_PROPERTIES, padding_index=hp.PADDING_INDEX, model_dimension=hp.MODEL_DIMENSION, target_sequence_length=hp.TARGET_SEQUENCE_LENGTH, dropout_probability=hp.DROPOUT_PROBABILITY, feed_forward_transformer_layer_dimension=hp.FEED_FORWARD_TRANSFORMER_LAYER_DIMENSION):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_dimension = model_dimension\n",
    "        \n",
    "        self.embedder = nn.Embedding(vocabulary_size, embedding_size, padding_idx=padding_index)\n",
    "        self.embedder_drouput = nn.Dropout(dropout_probability)\n",
    "        self.embedding_layer = nn.Sequential(self.embedder, self.embedder_drouput)\n",
    "\n",
    "        self.combine_embeddings_and_properties_layer = nn.Linear(embedding_size + number_of_properties, model_dimension)\n",
    "        self.positional_encoding_layer = PositionalEncoding(model_dimension, dropout_probability=dropout_probability)\n",
    "        self.transformer = nn.Transformer(d_model=model_dimension, dim_feedforward=feed_forward_transformer_layer_dimension, dropout=dropout_probability)\n",
    "        self.transformer_to_vocabulary_logits_layer = nn.Linear(model_dimension, vocabulary_size)\n",
    "\n",
    "        self.register_buffer('decoder_attention_mask', torch.full((target_sequence_length, target_sequence_length), float(\"-inf\")).triu(diagonal=1))\n",
    "\n",
    "    def forward(self, batched_source_numericalized, batched_source_properties, batched_target_input_numericalized, batched_source_padding_mask, batched_target_input_padding_mask):\n",
    "        # all needs to be torch.float, torch.double is not supported by Transformer()\n",
    "        # batched_source_numericalized -> batch_size x source_sequence_length(start_id, t1, t2, t3.., end_id, start_id, t4.., pad_id, pad_id..)(int/long)\n",
    "        # batched_source_properties -> batch_size x source_sequence_length x 5(number_of_properties)(0 for special tokens...)(float)\n",
    "        # batched_target_input_numericalized -> batch_size x target_sequence_length (start_id, t1, t2... pad_id, pad_id, pad_id...)(int/long)(without end_id)\n",
    "        # batched_source/target_padding_mask -> batch_size x source/target_sequence_length(False, False..., True, True..)(bool)\n",
    "\n",
    "        batched_source_embedded = self.embedding_layer(batched_source_numericalized)\n",
    "        batched_target_embedded = self.embedding_layer(batched_target_input_numericalized)\n",
    "        # batched_source/target_embedded -> batch_size x source/target_sequence_length x embedding_size\n",
    "\n",
    "        batched_source = torch.cat((batched_source_embedded, batched_source_properties), dim=2)  # batch_size x source_sequence_length x embedding_size + number_of_properies\n",
    "        batched_source = self.combine_embeddings_and_properties_layer(batched_source)  # batch_size x source_sequence_length x model_dimension\n",
    "\n",
    "        batched_source = self.positional_encoding_layer(batched_source * math.sqrt(self.model_dimension))  # normalizing(reducing variance) before positonally encoding\n",
    "        batched_target = self.positional_encoding_layer(batched_target_embedded * math.sqrt(self.model_dimension))\n",
    "        # batched_source/target -> batch_size x source/target_sequence_length x model_dimension\n",
    "\n",
    "        transformer_output = self.transformer(\n",
    "            src=rearrange(batched_source, 'b s m -> s b m'),  # batched_source.tranpose(0, 1)\n",
    "            tgt=rearrange(batched_target, 'b s m -> s b m'),\n",
    "            tgt_mask=self.decoder_attention_mask,\n",
    "            src_key_padding_mask=batched_source_padding_mask,\n",
    "            memory_key_padding_mask=batched_source_padding_mask,\n",
    "            tgt_key_padding_mask=batched_target_input_padding_mask,\n",
    "        )\n",
    "        transformer_output = rearrange(transformer_output, 's b m -> b s m')\n",
    "        # transformer_output -> batch_size x target_sequence_length x model_dimension\n",
    "\n",
    "        vocabulary_logits = self.transformer_to_vocabulary_logits_layer(transformer_output)\n",
    "        # vocabulary_logits -> batch_size x target_sequence_length x vocabulary_size\n",
    "\n",
    "        #vocabulary_log_probability = F.log_softmax(vocabulary_logits, dim=-1)\n",
    "        # it's more numerically stable\n",
    "        # https://deepdatascience.wordpress.com/2020/02/27/log-softmax-vs-softmax/\n",
    "\n",
    "        return vocabulary_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dimension, dropout_probability=0.1, cached_maximum_sequence_length = 5_000):\n",
    "        super().__init__()\n",
    "        self.model_dimension = model_dimension\n",
    "        self.cached_maximum_sequence_length = cached_maximum_sequence_length\n",
    "        \n",
    "        positional_encoding = self._compute_positional_encoding_matrix(model_dimension, cached_maximum_sequence_length)\n",
    "        # positional_encoding -> cached_maximum_sequence_length x model_dimension\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout_probability)\n",
    "        \n",
    "    def _compute_positional_encoding_matrix(self, model_dimension, sequence_length):\n",
    "        first_multiplier = torch.arange(0, sequence_length, dtype=torch.float)\n",
    "        second_multiplier = 1.0 / torch.pow(torch.tensor(10_000), torch.arange(0, model_dimension, 2, dtype=torch.float) / model_dimension)\n",
    "        computation = rearrange(first_multiplier, 'f -> f 1') * rearrange(second_multiplier, 's -> 1 s')\n",
    "        \n",
    "        positional_encoding = torch.zeros(sequence_length, model_dimension)\n",
    "        positional_encoding[:, ::2] = torch.sin(computation)\n",
    "        positional_encoding[:, 1::2] = torch.cos(computation)\n",
    "        \n",
    "        return positional_encoding\n",
    "        \n",
    "    def forward(self, batched_input):\n",
    "        # batched_input -> batch_size x sequence_length x model_dimension\n",
    "        positional_encoding = self.positional_encoding if (batched_input.shape[-2] <= self.cached_maximum_sequence_length) else self._compute_positional_encoding_matrix(self.model_dimension, batched_input.shape[-2])\n",
    "        \n",
    "        positionaly_encoded_input = batched_input + positional_encoding[:batched_input.shape[-2], :] \n",
    "        return self.dropout_layer(positionaly_encoded_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoding_layer = PositionalEncoding(model_dimension, cached_maximum_sequence_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding_layer.positional_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<super: <class 'Transformer'>, <Transformer object>>\n"
     ]
    }
   ],
   "source": [
    "t = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
